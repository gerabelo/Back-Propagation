***Back Propagation***  
  
Olá, talvez você ja tenha uma boa noção do que o Algoritmo Backpropagation é e esteja em dúvida apenas sobre a sua implementação em linguagem ANSI-C, então peço desculpas por este briefing.  
  
A Inteligência  Artificial, ao longo dos seus mais de 50 anos de existência, desenvolveu algumas ferramentas bastante interessantes. O mestrando finlandês, **Seppo Linnainmaa**, publicou, em 1970, um método utilizado em _Redes Neurais Artificiais_ (com Aprendizado Supervisionado) chamado _Automatic Differentiation_ que mais tarde ficou conhecido apenas por _Back Propagation_.  
O _BP_ baseia-se na minimização do erro proposta pelos matemáticos Cauchy, 1847, e Hadamard, 1908, conhecida por “_Gradiente Descendente_”, ajustando os pesos da rede através do cálculo da influência de cada neurônio nos erros obtidos durante a fase de treinamento.
    
    
***Aplicações***  
* Sonar target recognition (Gorman and Sejnowski, 1988);
* NETTalk (Sejnowski & Rosenberg, 1987 “Parallel Networks that Learn to Pronounce English Text”, Complex Systems 1, 145-168);
* Zipcode Recognition (Y. LeCun, 1990);
* ALVINN (Autonomous Land Vehicle In a Neural Network) (Pomerleau, 1996);
Face Recognition (Mitchell, 1997);
  
***Auto-ajuste***  

![equation](https://c1.staticflickr.com/5/4417/36199303024_8b1e924307_o.gif)  

![equation](https://c1.staticflickr.com/5/4374/36893529131_aa9591fd80_o.gif)  

***Gradiente Descendente***  
  
https://matheusfacure.github.io/2017/02/20/MQO-Gradiente-Descendente/
